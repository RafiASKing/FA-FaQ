# Claude's Analysis: EMR FAQ Retrieval System

**Author:** Claude (Opus 4.5)
**Date:** January 2026
**Scope:** Deep analysis of system architecture, owner's proposed features, and recommendations

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [System Understanding](#2-system-understanding)
3. [Analysis of Proposed Features](#3-analysis-of-proposed-features)
   - 3.1 [Retrieval Mode Selection](#31-retrieval-mode-selection-task_type-switching)
   - 3.2 [3-Level Intelligence Tiers](#32-3-level-intelligence-tiers-grader-agent)
   - 3.3 [Immediate Acknowledgment ("Baik mohon tunggu")](#33-immediate-acknowledgment-baik-mohon-tunggu)
   - 3.4 [FastAPI Admin Console Upgrade](#34-fastapi-admin-console-upgrade)
4. [Current System Strengths](#4-current-system-strengths)
5. [Current System Weaknesses & Gaps](#5-current-system-weaknesses--gaps)
6. [Additional Recommendations](#6-additional-recommendations)
7. [Implementation Priority Matrix](#7-implementation-priority-matrix)
8. [Final Verdict](#8-final-verdict)

---

## 1. Executive Summary

This EMR FAQ retrieval system is a **well-architected, production-ready** omnichannel knowledge base with semantic search capabilities. The codebase demonstrates mature engineering decisions: HyDE embedding format, retry-on-lock concurrency handling, clean separation between Streamlit-cached and raw API functions, and thoughtful UX patterns.

**Owner's proposed features assessment:**

| Feature | Verdict | Feasibility | Impact |
|---------|---------|-------------|--------|
| Retrieval Mode Selection | GOOD IDEA | High | Medium |
| 3-Level Intelligence Tiers | EXCELLENT IDEA | Medium-High | High |
| Immediate Acknowledgment | EXCELLENT IDEA | Very High | High |
| FastAPI Admin Console | SITUATIONAL | Medium | Low-Medium |

---

## 2. System Understanding

### 2.1 Architecture Overview

```
                           OMNICHANNEL DELIVERY
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Streamlit (8501/8502)  â”‚  FastAPI Web V2 (8080)  â”‚  WhatsApp Bot (8000)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                     â”‚                       â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  src/database.py        â”‚
                              â”‚  (Dual-mode access)     â”‚
                              â”‚  - Streamlit cached     â”‚
                              â”‚  - Raw API calls        â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  ChromaDB               â”‚
                              â”‚  Collection: faq_v1    â”‚
                              â”‚  + Gemini Embeddings    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Current Embedding Strategy

The system uses a **HyDE (Hypothetical Document Embeddings)** format:

```
DOMAIN: {tag} ({tag_description})
DOKUMEN: {judul}
VARIASI PERTANYAAN USER: {keyword}
ISI KONTEN: {cleaned_answer}
```

Currently, **ALL embeddings use `task_type="RETRIEVAL_DOCUMENT"`** (see `database.py:78`), which is actually **incorrect** for query-time embedding. This is a bug/oversight that your proposed feature would fix.

### 2.3 Search Flow

1. User query â†’ Generate embedding (currently using `RETRIEVAL_DOCUMENT`)
2. Pre-filter by tag (if selected) via ChromaDB `where` clause
3. Fetch top 50 candidates
4. Score: `(1 - distance) * 100`
5. Filter: score > 41%
6. Return top 3 (web) or top 1 (WhatsApp)

---

## 3. Analysis of Proposed Features

### 3.1 Retrieval Mode Selection (task_type switching)

**Your Proposal:**
```python
METHODS = {
    "RETRIEVAL_QUERY": {
        "label": "Asymmetric (Recommended)",
        "desc": "Fokus pada Intent, abaikan basa-basi.",
        "icon": "ðŸ›¡ï¸"
    },
    "RETRIEVAL_DOCUMENT": {
        "label": "Symmetric (Document)",
        "desc": "Mencocokkan pola kalimat & kata.",
        "icon": "ðŸ“„"
    },
    "SEMANTIC_SIMILARITY": {
        "label": "Semantic Similarity",
        "desc": "Kemiripan makna murni.",
        "icon": "âš–ï¸"
    },
    "QUESTION_ANSWERING": {
        "label": "Question Answering",
        "desc": "Ekspektasi kalimat tanya baku.",
        "icon": "ðŸŽ“"
    }
}
```

#### My Verdict: GOOD IDEA (with caveats)

**Why it's good:**
1. **Fixes a current bug**: Your system currently uses `RETRIEVAL_DOCUMENT` for BOTH document storage AND query embedding. According to Google's documentation, this is wrong. Documents should use `RETRIEVAL_DOCUMENT`, but queries should use `RETRIEVAL_QUERY` for asymmetric retrieval.

2. **Scientifically backed**: Google claims using correct task types can improve search relevance by **10-30%**.

3. **Low implementation cost**: Just add a parameter to `_generate_embedding_raw()` and `search_faq()`.

**Caveats & Concerns:**

1. **Admin confusion risk**: Most admins won't understand the difference. The descriptions you provided are good, but still abstract.

2. **Asymmetric mismatch problem**: If admin selects `SEMANTIC_SIMILARITY` for queries but documents are stored with `RETRIEVAL_DOCUMENT`, there's an embedding space mismatch. This could degrade results.

3. **`QUESTION_ANSWERING` is not what you think**: According to Google docs, this is meant for queries phrased as proper questions ("Why is X?", "How do I Y?"). Your users type things like "gabisa masuk", "error discharge", "gimana edit obat" - these are NOT formal questions. `RETRIEVAL_QUERY` handles this better.

**My recommendation:**

```python
# Simpler, safer design
METHODS = {
    "RETRIEVAL_QUERY": {
        "label": "Standard (Recommended)",
        "desc": "Cocok untuk keyword pendek & bahasa panik user.",
        "icon": "ðŸ”"
    },
    "QUESTION_ANSWERING": {
        "label": "Formal Question",
        "desc": "Untuk kalimat tanya lengkap (Bagaimana cara...?)",
        "icon": "â“"
    }
}
# Remove SEMANTIC_SIMILARITY and RETRIEVAL_DOCUMENT from query options
# Those are NOT meant for retrieval queries per Google docs
```

**Implementation sketch:**

```python
# database.py
def _generate_embedding_raw(text, task_type="RETRIEVAL_DOCUMENT"):
    client = _get_ai_client_raw()
    response = client.models.embed_content(
        model="models/gemini-embedding-001",
        contents=text,
        config=types.EmbedContentConfig(task_type=task_type)
    )
    return response.embeddings[0].values

def search_faq_for_bot(query_text, filter_tag="Semua Modul", task_type="RETRIEVAL_QUERY"):
    # ... existing code ...
    vec = _generate_embedding_raw(query_text, task_type=task_type)  # Pass task_type
    # ...
```

**Feasibility: HIGH** - 2-4 hours of work

---

### 3.2 3-Level Intelligence Tiers (Grader Agent)

**Your Proposal:**
```
Level 1: Pure embedding retrieval (current system)
Level 2: Grader agent using Gemini 2.5 Flash - evaluates relevance, chooses best FAQ
Level 3: Grader agent using Gemini 2.5 Pro - smarter evaluation, higher top_k
```

#### My Verdict: EXCELLENT IDEA

This is the most impactful feature you proposed. Here's my deep analysis:

**Why it's excellent:**

1. **Addresses the "false positive" problem**: Pure embedding search can return high-scoring but semantically wrong results. A grader LLM can understand context and reject mismatches.

2. **Tiered cost structure**: You pay for intelligence when needed. Most queries work fine with Level 1. Complex/ambiguous queries get Level 2/3.

3. **Multi-FAQ response capability**: Your idea of returning 2-3 FAQs when uncertain is smart. WhatsApp users often ask vague questions - giving options is better UX.

4. **Industry standard**: This is essentially a lightweight RAG + Re-ranking pattern used by production systems.

**Detailed Design Recommendation:**

```python
# config.py - New constants
INTELLIGENCE_LEVELS = {
    "LEVEL_1": {
        "label": "Fast (Embedding Only)",
        "desc": "Cepat, murah. Cocok untuk pertanyaan jelas.",
        "model": None,  # No LLM call
        "top_k": 3,
        "cost_per_query": 0,  # Just embedding cost
        "avg_latency_ms": 200
    },
    "LEVEL_2": {
        "label": "Smart (Flash Grader)",
        "desc": "LLM menilai relevansi. Lebih akurat.",
        "model": "gemini-2.5-flash-preview-05-20",
        "top_k": 5,
        "cost_per_query": 0.001,  # Approximate
        "avg_latency_ms": 800
    },
    "LEVEL_3": {
        "label": "Expert (Pro Grader)",
        "desc": "Model terbaik. Untuk kasus kompleks.",
        "model": "gemini-2.5-pro-preview-06-05",
        "top_k": 10,
        "cost_per_query": 0.01,  # Approximate
        "avg_latency_ms": 2000
    }
}
```

**Grader Prompt Design (Critical for success):**

```python
GRADER_SYSTEM_PROMPT = """Anda adalah sistem penilaian relevansi FAQ rumah sakit.

TUGAS: Nilai apakah FAQ kandidat menjawab pertanyaan user.

KRITERIA PENILAIAN:
- RELEVANT: FAQ langsung menjawab pertanyaan user
- PARTIAL: FAQ terkait tapi tidak langsung menjawab
- IRRELEVANT: FAQ tidak ada hubungannya

OUTPUT FORMAT (JSON):
{
  "evaluations": [
    {"faq_id": "1", "verdict": "RELEVANT", "confidence": 0.95, "reason": "..."},
    {"faq_id": "2", "verdict": "PARTIAL", "confidence": 0.60, "reason": "..."}
  ],
  "best_match": "1",
  "should_return_multiple": false
}

PENTING:
- Jika tidak ada yang RELEVANT, set best_match = null
- Jika ada 2+ dengan confidence serupa, set should_return_multiple = true
"""

GRADER_USER_TEMPLATE = """
PERTANYAAN USER: {user_query}

FAQ KANDIDAT:
{candidates_formatted}

Evaluasi setiap FAQ dan pilih yang terbaik.
"""
```

**Implementation Architecture:**

```python
# New file: src/grader.py

import json
from google import genai
from .config import INTELLIGENCE_LEVELS, GOOGLE_API_KEY

class FAQGrader:
    def __init__(self, level: str = "LEVEL_2"):
        self.config = INTELLIGENCE_LEVELS[level]
        self.client = genai.Client(api_key=GOOGLE_API_KEY)

    def grade_candidates(self, user_query: str, candidates: list) -> dict:
        """
        Evaluates FAQ candidates using LLM.

        Returns:
            {
                "best_faq": {...} or None,
                "alternatives": [...],  # If uncertain, return 2-3
                "confidence": 0.85,
                "reasoning": "..."
            }
        """
        if not self.config["model"]:
            # Level 1: Just return top embedding match
            return {"best_faq": candidates[0] if candidates else None, "alternatives": []}

        # Format candidates for prompt
        formatted = self._format_candidates(candidates)

        response = self.client.models.generate_content(
            model=self.config["model"],
            contents=[
                {"role": "user", "parts": [{"text": GRADER_SYSTEM_PROMPT}]},
                {"role": "user", "parts": [{"text": GRADER_USER_TEMPLATE.format(
                    user_query=user_query,
                    candidates_formatted=formatted
                )}]}
            ],
            config={"response_mime_type": "application/json"}
        )

        return self._parse_response(response, candidates)

    def _format_candidates(self, candidates):
        # Format each FAQ for the prompt
        lines = []
        for i, c in enumerate(candidates):
            lines.append(f"[FAQ {c['id']}] {c['judul']}\n{c['jawaban_tampil'][:500]}...\n")
        return "\n".join(lines)

    def _parse_response(self, response, candidates):
        # Parse LLM JSON output and map back to FAQ objects
        # ... implementation details
        pass
```

**Modified WhatsApp Bot Flow:**

```python
# bot_wa.py - Modified process_logic

async def process_logic(remote_jid, sender_name, message_body, is_group, mentioned_list, intelligence_level="LEVEL_1"):
    # ... existing validation code ...

    # Step 1: Get embedding candidates (always)
    results = database.search_faq_for_bot(clean_query, n_results=INTELLIGENCE_LEVELS[intelligence_level]["top_k"])

    # Step 2: Apply grader if Level 2 or 3
    if intelligence_level in ["LEVEL_2", "LEVEL_3"]:
        grader = FAQGrader(level=intelligence_level)
        candidates = format_results_for_grader(results)
        graded = grader.grade_candidates(clean_query, candidates)

        if graded["best_faq"] is None:
            # Grader says none are relevant
            send_wpp_text(remote_jid, "Maaf, tidak ditemukan jawaban yang cocok...")
            return

        if graded.get("alternatives") and len(graded["alternatives"]) > 0:
            # Return multiple FAQs
            send_multiple_faqs(remote_jid, graded["best_faq"], graded["alternatives"])
            return

        # Single best match
        meta = graded["best_faq"]
    else:
        # Level 1: Use embedding result directly
        meta = results['metadatas'][0][0]

    # ... rest of sending logic ...
```

**Cost Analysis:**

| Level | Estimated Cost/Query | Monthly (10k queries) |
|-------|---------------------|----------------------|
| Level 1 | $0.0001 (embedding only) | $1 |
| Level 2 | $0.001 (Flash ~500 tokens) | $10 |
| Level 3 | $0.01 (Pro ~500 tokens) | $100 |

**Feasibility: MEDIUM-HIGH** - 1-2 days of focused work

**Risks:**
1. **Latency**: Level 3 can add 2+ seconds. Need the "mohon tunggu" feature.
2. **Prompt engineering**: Grader quality depends heavily on prompt design. Expect iteration.
3. **JSON parsing**: LLMs sometimes break JSON format. Need robust error handling.

---

### 3.3 Immediate Acknowledgment ("Baik mohon tunggu")

**Your Proposal:**
> "the first time tagged immediately say 'baik mohon tunggu' while process things in the background"

#### My Verdict: EXCELLENT IDEA

This is crucial UX, especially if you implement the grader tiers (which add latency).

**Why it's excellent:**

1. **Perceived performance**: Users feel the system is responsive even if actual response takes 2-5 seconds.

2. **WhatsApp UX standard**: Popular bots (OVO, Gojek CS) all do this. Users expect it.

3. **Prevents "is it broken?" follow-up messages**: Without ack, users spam the bot thinking it's dead.

**Implementation:**

```python
# bot_wa.py - Modified webhook handler

@app.post("/webhook")
async def wpp_webhook(request: Request, background_tasks: BackgroundTasks):
    try:
        body = await request.json()
        event = body.get("event")

        if event not in ["onMessage", "onAnyMessage", "onmessage"]:
            return {"status": "ignored_event"}

        data = body.get("data") or body
        if data.get("fromMe", False):
            return {"status": "ignored_self"}

        remote_jid = data.get("from") or data.get("chatId")
        if not remote_jid or "status@broadcast" in str(remote_jid):
            return {"status": "ignored_status"}

        message_body = data.get("body") or ""
        is_group = data.get("isGroupMsg", False) or "@g.us" in str(remote_jid)

        # === NEW: Quick validation for should_reply ===
        should_reply = should_process_message(message_body, is_group, data.get("mentionedJidList", []))

        if should_reply:
            # === IMMEDIATE ACKNOWLEDGMENT ===
            sender_name = data.get("sender", {}).get("pushname", "")
            ack_message = f"Baik {sender_name}, mohon tunggu sebentar... ðŸ”"
            send_wpp_text(remote_jid, ack_message)  # Sync call, fast

        # Background processing (actual FAQ search)
        background_tasks.add_task(
            process_logic,
            remote_jid,
            data.get("sender", {}).get("pushname", "User"),
            message_body,
            is_group,
            data.get("mentionedJidList", [])
        )

        return {"status": "success"}

    except Exception as e:
        log(f"Webhook Error: {e}")
        return {"status": "error"}


def should_process_message(message_body: str, is_group: bool, mentioned_list: list) -> bool:
    """
    Quick check if we should respond to this message.
    Extracted from process_logic for early ack decision.
    """
    if not is_group:
        return True  # Always reply to DMs

    if "@faq" in message_body.lower():
        return True

    if mentioned_list:
        for mentioned_id in mentioned_list:
            for my_id in MY_IDENTITIES:
                if str(my_id) in str(mentioned_id):
                    return True

    return False
```

**Enhancement - Typing Indicator:**

For even better UX, you can show "typing..." indicator:

```python
def send_typing_indicator(phone: str, duration_ms: int = 3000):
    """Shows 'typing...' bubble in WhatsApp"""
    url = f"{WA_BASE_URL}/api/{WA_SESSION_NAME}/start-typing"
    payload = {"phone": phone, "duration": duration_ms}
    try:
        requests.post(url, json=payload, headers=get_headers())
    except:
        pass  # Non-critical, ignore failures

# Usage in webhook:
if should_reply:
    send_typing_indicator(remote_jid, 5000)  # Show typing for 5 sec
    send_wpp_text(remote_jid, "Baik, mohon tunggu...")
```

**Feasibility: VERY HIGH** - 30 minutes to 1 hour

---

### 3.4 FastAPI Admin Console Upgrade

**Your Proposal:**
> "maybe a better version of admin console like query web from streamlit to fastapi version"

#### My Verdict: SITUATIONAL (Not a priority)

**Arguments FOR:**
1. **Performance**: FastAPI is faster than Streamlit for concurrent admin users
2. **Professional look**: Custom frontend can look more polished
3. **Flexibility**: Full control over UI/UX, no Streamlit constraints

**Arguments AGAINST:**

1. **Streamlit is actually great for admin tools**: The current admin.py has excellent UX:
   - Tabs for organization
   - Live preview
   - Smart toolbar with auto-counter
   - Draft preservation
   - DataFrame views
   - Backup/restore

   Recreating all this in FastAPI+HTML would take **5-10x longer**.

2. **Admin console doesn't need to scale**: It's used by 1-3 people internally. Streamlit handles this fine.

3. **Iteration speed**: With Streamlit, you can add features in minutes. With FastAPI+Jinja2, every change needs HTML/CSS/JS updates.

4. **Current pain points are minor**: The only real limitation is Streamlit's refresh behavior, which is manageable.

**When you SHOULD migrate:**

- If you have 10+ concurrent admin users
- If you need complex client-side interactivity (drag-drop reordering, inline editing)
- If you're building a multi-tenant SaaS product

**My recommendation:**

Keep Streamlit for admin. Instead, invest time in the **grader feature** and **retrieval mode selection** - these have much higher ROI.

If you MUST improve admin UX, consider these lower-effort options:

1. **Streamlit + st-aggrid**: Add Excel-like inline editing
2. **Streamlit + streamlit-antd-components**: Better UI components
3. **Streamlit multi-page apps**: Split into dedicated pages instead of tabs

**Feasibility: MEDIUM** (if attempted)
**ROI: LOW** (compared to other features)

---

## 4. Current System Strengths

1. **HyDE Embedding Format**: Brilliant design that bridges user "panic language" with formal documentation.

2. **Dual-mode Database Access**: Clean separation between `@st.cache` functions and raw API functions. This allows WhatsApp bot to work without Streamlit dependencies.

3. **Retry-on-Lock Decorator**: Handles SQLite concurrency gracefully with jitter backoff.

4. **Image Handling Pipeline**: Auto-compression, organized storage by tag, inline `[GAMBAR X]` syntax is intuitive for admins.

5. **Analytics Feedback Loop**: Failed search logging drives content creation decisions.

6. **Configuration as Code**: `tags_config.json` is admin-editable without code changes.

7. **Multi-Channel Consistency**: Same search logic across web, admin, and WhatsApp.

---

## 5. Current System Weaknesses & Gaps

### 5.1 Bug: Wrong task_type for Query Embedding

**Location**: `database.py:78`

```python
# Current (WRONG):
config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT")

# Should be:
# For storing documents: task_type="RETRIEVAL_DOCUMENT"  (correct)
# For searching queries: task_type="RETRIEVAL_QUERY"    (currently missing!)
```

**Impact**: 10-30% potential relevance improvement lost.

**Fix**: Parameterize `task_type` in `_generate_embedding_raw()` and use `RETRIEVAL_QUERY` for search functions.

### 5.2 No Query Preprocessing

User input goes directly to embedding. Consider:

```python
def preprocess_query(raw_query: str) -> str:
    # Remove common filler words
    query = raw_query.lower()
    fillers = ["tolong", "minta", "gimana", "caranya", "dong", "ya", "kak"]
    for f in fillers:
        query = query.replace(f, "")
    return query.strip()
```

### 5.3 No Caching for WhatsApp Bot

`search_faq_for_bot()` creates new connections every call. Consider connection pooling or TTL cache:

```python
from functools import lru_cache
import time

@lru_cache(maxsize=100)
def cached_embedding(query: str, timestamp_bucket: int):
    """Cache embeddings for 5 minutes"""
    return _generate_embedding_raw(query, task_type="RETRIEVAL_QUERY")

# Usage:
timestamp_bucket = int(time.time() // 300)  # 5-minute buckets
vec = cached_embedding(clean_query, timestamp_bucket)
```

### 5.4 No Rate Limiting on WhatsApp Bot

A malicious user could spam the bot, incurring Gemini API costs. Add:

```python
from collections import defaultdict
import time

RATE_LIMITS = defaultdict(list)  # phone -> [timestamp, ...]

def is_rate_limited(phone: str, max_per_minute: int = 5) -> bool:
    now = time.time()
    RATE_LIMITS[phone] = [t for t in RATE_LIMITS[phone] if now - t < 60]
    if len(RATE_LIMITS[phone]) >= max_per_minute:
        return True
    RATE_LIMITS[phone].append(now)
    return False
```

### 5.5 No Observability

No structured logging, metrics, or tracing. Consider:

- **Structured logs**: JSON format with query, latency, score, result_id
- **Prometheus metrics**: Query latency histogram, success/fail counters
- **Error tracking**: Sentry integration for production

---

## 6. Additional Recommendations

### 6.1 Implement Query Expansion (Low effort, high impact)

Before searching, expand query with synonyms:

```python
SYNONYMS = {
    "login": ["masuk", "sign in", "autentikasi"],
    "discharge": ["pulang", "checkout", "keluar"],
    "edit": ["ubah", "ganti", "update", "modifikasi"],
    "obat": ["farmasi", "pharmacy", "resep", "medicine"],
}

def expand_query(query: str) -> str:
    expanded = query
    for term, syns in SYNONYMS.items():
        if term in query.lower():
            expanded += " " + " ".join(syns)
        for syn in syns:
            if syn in query.lower():
                expanded += " " + term
    return expanded
```

### 6.2 Add Negative Examples to HyDE

Your current HyDE format tells AI what the document IS. Also tell it what it's NOT:

```
DOMAIN: ED (IGD, Emergency)
DOKUMEN: Cara Login EMR
VARIASI PERTANYAAN USER: Gak bisa masuk, lupa password
BUKAN TENTANG: Cara logout, reset password admin, registrasi pasien baru
ISI KONTEN: ...
```

### 6.3 Implement Fallback to Web Search

When local FAQ has no good match, offer Google search:

```python
if score < 41:
    google_query = urllib.parse.quote(f"EMR rumah sakit {clean_query}")
    msg = f"Tidak ditemukan di database internal.\n\nCoba cari di Google:\nhttps://google.com/search?q={google_query}"
```

### 6.4 A/B Testing Framework

To validate improvements (like task_type change), implement simple A/B:

```python
import random

def get_search_config(user_id: str) -> dict:
    # Deterministic assignment based on user_id
    bucket = hash(user_id) % 100
    if bucket < 50:
        return {"variant": "A", "task_type": "RETRIEVAL_DOCUMENT"}
    else:
        return {"variant": "B", "task_type": "RETRIEVAL_QUERY"}
```

Log results and compare relevance metrics.

---

## 7. Implementation Priority Matrix

| Priority | Feature | Effort | Impact | Dependencies |
|----------|---------|--------|--------|--------------|
| **P0** | Fix task_type bug | 1 hour | High | None |
| **P0** | Immediate acknowledgment | 1 hour | High | None |
| **P1** | Retrieval mode selection (simplified) | 4 hours | Medium | P0 |
| **P1** | Level 2 grader (Flash) | 1 day | High | P0 |
| **P2** | Level 3 grader (Pro) | 4 hours | Medium | P1 |
| **P2** | Multi-FAQ response for WhatsApp | 4 hours | Medium | P1 |
| **P3** | Rate limiting | 2 hours | Low | None |
| **P3** | Query preprocessing | 2 hours | Low | None |
| **P4** | FastAPI admin console | 1-2 weeks | Low | None |

---

## 8. Final Verdict

### Your Ideas - Ranked

1. **Immediate Acknowledgment**: DO THIS FIRST. Easiest to implement, biggest UX win.

2. **3-Level Intelligence Tiers**: This is the strategic differentiator. Start with Level 2 (Flash) as default, add Level 3 later. Consider making Level 1 (pure embedding) the "lite" option.

3. **Retrieval Mode Selection**: Good idea but simplify. Only expose `RETRIEVAL_QUERY` (default) and `QUESTION_ANSWERING`. Remove the others - they'll confuse admins.

4. **FastAPI Admin Console**: Skip for now. Streamlit is fine. Focus engineering effort on user-facing improvements.

### Overall Assessment

Your codebase is solid. The ideas you proposed show good product thinking - you're addressing real user pain points (latency perception, relevance accuracy).

The grader agent is particularly innovative. Most FAQ systems stop at embedding similarity. Adding an LLM "judge" elevates this to near-conversational AI quality while keeping costs manageable.

**Key Success Metrics to Track:**

1. **Search success rate**: % of queries with score > 60%
2. **User satisfaction**: Track if users ask follow-up questions (bad) or say "terima kasih" (good)
3. **Cost per query**: Monitor Gemini API spend by intelligence level
4. **Latency P95**: Keep under 5 seconds for Level 3

---

*Document generated by Claude (Opus 4.5) after analyzing the complete codebase.*

---

## Appendix: Research Sources

- [Google Gemini Embedding Task Types](https://ai.google.dev/gemini-api/docs/embeddings)
- [Vertex AI Task Type Documentation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types)
- [Google Cloud Blog: Improve Search with Task Types](https://cloud.google.com/blog/products/ai-machine-learning/improve-gen-ai-search-with-vertex-ai-embeddings-and-task-types)
- [Understanding Task Types in Gemini Embedding API](https://technicalwriting.dev/embeddings/tasks/index.html)
- [Gemini API Models & Pricing](https://ai.google.dev/gemini-api/docs/pricing)
